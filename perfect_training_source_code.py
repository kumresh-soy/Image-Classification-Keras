# -*- coding: utf-8 -*-
"""perfect training source code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rp12MY2WHbLUdnxyjSmfXidQLDVQfDkg

To make this project i am using Below Requirement: 


1.   Necessary libary like Keras, tensorflow, numpy, etc.
2.   Jupyter notebook but i am using google coab for traing 
3.   TensorFlow is a free and open-source software library for machine learning. so it is easy to implement.

## **Importing necessary Libary**
In this project i am using Tensorflow and keras for machine `learning`
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import os

"""Here we are loading data from gdrive and extract it."""

from zipfile import ZipFile
file_name = '/content/drive/MyDrive/dataset/PlantDataset.zip'
#extracting zip filez
with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print("done")

# # importing required modules
# from zipfile import ZipFile
# import os

# def get_all_file_paths(directory):

# 	# initializing empty file paths list
# 	file_paths = []

# 	# crawling through directory and subdirectories
# 	for root, directories, files in os.walk(directory):
# 		for filename in files:
# 			# join the two strings in order to form the full filepath.
# 			filepath = os.path.join(root, filename)
# 			file_paths.append(filepath)

# 	# returning all file paths
# 	return file_paths		

# def main():
# 	# path to folder which needs to be zipped
# 	directory = '/content/drive/MyDrive/dataset/Plant Detaset'

# 	# calling function to get all file paths in the directory
# 	file_paths = get_all_file_paths(directory)

# 	# writing files to a zipfile
# 	with ZipFile('PlantDataset.zip','w') as zip:
# 		# writing each file one by one
# 		for file in file_paths:
# 			zip.write(file)

# 	print('All files zipped successfully!')		


# main()

# !cp PlantDataset.zip "/content/drive/MyDrive/dataset"

data_dir = os.path.join(os.path.dirname("/content/content/drive/MyDrive/dataset/Plant Detaset"), 'Plant Detaset')
train_dir = os.path.join(data_dir, 'train')

import time
import os
from os.path import exists

def count(dir, counter=0):
    "returns number of files in dir and subdirs"
    for pack in os.walk(dir):
        for f in pack[2]:
            counter += 1
    return dir + " : " + str(counter) + "files"

"""Printing total no of images each folder have."""

print('total images for training :', count(train_dir))

BATCH_SIZE =  28#@param {type:"integer"}

IMAGE_SIZE = (224, 224)

train_detegen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_ds = train_detegen.flow_from_directory(
    train_dir,
    subset="training",
    seed=133,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)
val_ds = train_detegen.flow_from_directory(
    train_dir,
    subset="validation",
    seed=133,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
)

"""# Fine-tuning a pre-trained model: 

To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.

# Compile the model

Compile the model before training it. Since there are two classes, use a binary `cross-entropy` loss with `from_logits=True` since the model provides a linear output.
"""

class_num = len(train_ds.class_indices)
class_num

base_model = keras.applications.MobileNet(input_shape=(224, 224, 3) ,include_top=False, weights='imagenet')
x = base_model.output
# x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(1024, activation='relu')(x)
predictions = layers.Dense(class_num, activation="softmax")(x)
model = keras.Model(inputs=base_model.input, outputs=predictions)

"""# Continue training the model
If you trained to convergence earlier, this step will improve your accuracy by a few percentage points.
"""

for layer in base_model.layers:
  layer.trainable = False

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 7
model.fit(
    train_ds, epochs=epochs,validation_data=val_ds,
)

classes = list(train_ds.class_indices)
classes

from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'

# returns a compiled model
# identical to the previous one

!pip install tensorflowjs

# Python

import tensorflowjs as tfjs

tfjs.converters.save_keras_model(model,"/content/content")

!tensorflowjs_converter --input_format=keras /content/my_model.h5 /content/content/drive

"""# Graph 
Accuracy and loss

Createing load_image and prdict function
"""

# Import OpenCV
import cv2
import matplotlib.pylab as plt
import numpy as np

import itertools
import random
from collections import Counter
from glob import iglob


def load_image(filename):
    img = cv2.imread(os.path.join(data_dir, train_dir, filename))
    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]) )
    img = img /255
    
    return img


def predict(image):
    probabilities = model.predict(np.asarray([img]))[0]
    class_idx = np.argmax(probabilities)
    
    return {classes[class_idx]: probabilities[class_idx]}

for idx, filename in enumerate(random.sample(val_ds.filenames, 5)):
    print("SOURCE: class: %s, file: %s" % (os.path.split(filename)[0], filename))
    
    img = load_image(filename)
    prediction = predict(img)
    print("PREDICTED: class: %s, confidence: %f" % (list(prediction.keys())[0], list(prediction.values())[0]))
    plt.imshow(img)
    plt.figure(idx)    
    plt.show()

"""Printing and showing image with labels """

# Convert Keras model to TF Lite format.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_float_model = converter.convert()

# Show model size in MBs.
float_model_size = len(tflite_float_model) / (1024 * 1024)
print('Float model size = %dMBs.' % float_model_size)

f = open('plant_Dataset.tflite', "wb")
f.write(tflite_float_model)
f.close()

!ls "plant_Dataset.tflite" -lh